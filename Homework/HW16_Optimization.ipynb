{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 16.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 16.4\n",
    "f = lambda x: cos(x)\n",
    "df = lambda x: -sin(x)\n",
    "d2f = lambda x: -cos(x)\n",
    "x0 = 0.25\n",
    "x1 = x0 - df(x0)/d2f(x0)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(formula, symbols, values=None):\n",
    "    '''\n",
    "    Given a SymPy formula and variables\n",
    "    Find its analytic gradient without substituion\n",
    "    as a list of SymPy formulae or numerical gradient\n",
    "    if values specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    gradient = []\n",
    "    for i in range(size):\n",
    "        gradient.append(formula.diff(symbols[i]))\n",
    "        \n",
    "    if values == None:\n",
    "        return gradient\n",
    "\n",
    "    # Make sure you don't mess up the analytical gradient\n",
    "    g_copy = gradient.copy()\n",
    "    gradient_at = []\n",
    "    for i in range(len(g_copy)):\n",
    "        for j in range(len(symbols)):\n",
    "            g_copy[i] = g_copy[i].subs(symbols[j], values[j])\n",
    "        gradient_at.append(float(g_copy[i].evalf()))\n",
    "    return gradient_at\n",
    "\n",
    "def subs_all(formula, variables, values):\n",
    "    '''\n",
    "    You know what, it's getting to the point where this function\n",
    "    is necessary\n",
    "    '''\n",
    "    result = formula\n",
    "    for i in range(len(values)):\n",
    "        result = result.subs(variables[i], values[i])\n",
    "    return float(result.evalf())\n",
    "\n",
    "def hessian(gradient, symbols, values=None):\n",
    "    '''\n",
    "    Given an analytic gradient and variables\n",
    "    Calculate its analytic Hessian\n",
    "    or numerical Hessian if values are specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    hessian = []\n",
    "    for i in range(size):\n",
    "        hessian.append([0]*size)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian[i][j] = gradient[i].diff(symbols[j])\n",
    "            \n",
    "    if values == None:\n",
    "        return hessian\n",
    "    \n",
    "    hessian_at = hessian.copy()\n",
    "    size = len(hessian)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian_at[i][j] = subs_all(hessian_at[i][j], symbols, [float(v) for v in values])\n",
    "#             for k in range(len(symbols)):\n",
    "#                 hessian_at[i][j] = hessian_at[i][j].subs(symbols[k], values[k])\n",
    "#             hessian_at[i][j] = float(hessian_at[i][j].evalf())\n",
    "    return hessian_at\n",
    "\n",
    "def newton_nd_optimization_crude_step(formula, symbols, x_prev):\n",
    "    x_prev = list(x_prev)\n",
    "    grad = gradient(formula, symbols)\n",
    "    neg_gradient_at = -1 * np.array(gradient(formula, symbols, values=x_prev))\n",
    "    print(neg_gradient_at)\n",
    "    hes_at = np.array(hessian(grad, symbols, values=x_prev))\n",
    "    print(hes_at)\n",
    "    print(np.array(x_prev) + la.solve(hes_at, neg_gradient_at))\n",
    "    return np.array(x_prev) + la.solve(hes_at, neg_gradient_at)\n",
    "\n",
    "def newton_nd_optimization_crude(f_str, s_str, start, tolerance, actual_solution):\n",
    "    '''\n",
    "    A crude version of Newton ND Optimization algorithm\n",
    "    Maybe you can help out implementing an analytic solution\n",
    "    Returning the numerical solution as well as the number of\n",
    "    iterations it took\n",
    "    '''\n",
    "    formula = sp.sympify(f_str)\n",
    "    symbols = sp.symbols(s_str)\n",
    "    curr = np.copy(start)\n",
    "    iterations = 0\n",
    "    while (la.norm(curr - actual_solution, 2) > tolerance):\n",
    "        curr = newton_nd_optimization_crude_step(formula, symbols, curr)\n",
    "        iterations += 1\n",
    "    return curr, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 16.10\n",
    "newton_nd_optimization_crude(f_str = '20*x**2-6*x+6+20*y**2-x*y', \n",
    "                             s_str = 'x y', \n",
    "                             start = [1, 2], \n",
    "                             tolerance = 1e-10,\n",
    "                             actual_solution = [240/1599, 6/1599])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 16.11\n",
    "newton_nd_optimization_crude(f_str = '3*x**2+2*x*y+6*y**2', \n",
    "                             s_str = 'x y', \n",
    "                             start = [9, 6], \n",
    "                             tolerance = 1,\n",
    "                             actual_solution = [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 16.12\n",
    "newton_nd_optimization_crude(f_str = '2*x**2+11*x*y+2*y**2+6*E**(14*x*y)+9*sin(y)**2+9*cos(x*y)', \n",
    "                             s_str = 'x y', \n",
    "                             start = [0, 1], \n",
    "                             tolerance = 1e-10,\n",
    "                             actual_solution = [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW 16.15\n",
    "\n",
    "from math import sin, cos\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import sympy as sp\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(alpha, r, s):\n",
    "    r += s*alpha\n",
    "    x, y = r\n",
    "    return 3 +((x**2)/8) + ((y**2)/8) - np.sin(x)*np.cos((2**-0.5)*y)\n",
    "\n",
    "def sd_nd_optimization_crude_step(formula, symbols, x_prev):\n",
    "    x_prev = list(x_prev)\n",
    "    neg_gradient_at = -1 * np.array(gradient(formula, symbols, values=x_prev))\n",
    "    alpha = opt.minimize_scalar(f, args=(x_prev, neg_gradient_at)).x\n",
    "    return np.array(x_prev) + alpha * neg_gradient_at, la.norm(neg_gradient_at, 2)\n",
    "\n",
    "def gradient(formula, symbols, values=None):\n",
    "    '''\n",
    "    Given a SymPy formula and variables\n",
    "    Find its analytic gradient without substituion\n",
    "    as a list of SymPy formulae or numerical gradient\n",
    "    if values specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    gradient = []\n",
    "    for i in range(size):\n",
    "        gradient.append(formula.diff(symbols[i]))\n",
    "        \n",
    "    if values == None:\n",
    "        return gradient\n",
    "\n",
    "    # Make sure you don't mess up the analytical gradient\n",
    "    g_copy = gradient.copy()\n",
    "    gradient_at = []\n",
    "    for i in range(len(g_copy)):\n",
    "        for j in range(len(symbols)):\n",
    "            g_copy[i] = g_copy[i].subs(symbols[j], values[j])\n",
    "        gradient_at.append(float(g_copy[i].evalf()))\n",
    "    return gradient_at\n",
    "\n",
    "def subs_all(formula, variables, values):\n",
    "    '''\n",
    "    You know what, it's getting to the point where this function\n",
    "    is necessary\n",
    "    '''\n",
    "    result = formula\n",
    "    for i in range(len(values)):\n",
    "        result = result.subs(variables[i], values[i])\n",
    "    return float(result.evalf())\n",
    "\n",
    "def hessian(gradient, symbols, values=None):\n",
    "    '''\n",
    "    Given an analytic gradient and variables\n",
    "    Calculate its analytic Hessian\n",
    "    or numerical Hessian if values are specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    hessian = []\n",
    "    for i in range(size):\n",
    "        hessian.append([0]*size)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian[i][j] = gradient[i].diff(symbols[j])\n",
    "            \n",
    "    if values == None:\n",
    "        return hessian\n",
    "    \n",
    "    hessian_at = hessian.copy()\n",
    "    size = len(hessian)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian_at[i][j] = subs_all(hessian_at[i][j], symbols, [float(v) for v in values])\n",
    "#             for k in range(len(symbols)):\n",
    "#                 hessian_at[i][j] = hessian_at[i][j].subs(symbols[k], values[k])\n",
    "#             hessian_at[i][j] = float(hessian_at[i][j].evalf())\n",
    "    return hessian_at\n",
    "\n",
    "def newton_nd_optimization_crude_step(formula, symbols, x_prev):\n",
    "    x_prev = list(x_prev)\n",
    "    grad = gradient(formula, symbols)\n",
    "    neg_gradient_at = -1 * np.array(gradient(formula, symbols, values=x_prev))\n",
    "    hes_at = np.array(hessian(grad, symbols, values=x_prev))\n",
    "    return np.array(x_prev) + la.solve(hes_at, neg_gradient_at), la.norm(neg_gradient_at, 2)\n",
    "\n",
    "def newton_nd_optimization_crude(f_str, s_str, start, tolerance):\n",
    "    '''\n",
    "    A crude version of Newton ND Optimization algorithm\n",
    "    Maybe you can help out implementing an analytic solution\n",
    "    Returning the numerical solution as well as the number of\n",
    "    iterations it took\n",
    "    '''\n",
    "    x_list = []\n",
    "    formula = sp.sympify(f_str)\n",
    "    symbols = sp.symbols(s_str)\n",
    "    curr = np.copy(start)\n",
    "    iterations = 0\n",
    "    gradient = np.inf\n",
    "    while (gradient > tolerance):\n",
    "        x_list.append(curr)\n",
    "        curr, gradient = newton_nd_optimization_crude_step(formula, symbols, curr)\n",
    "        _, gradient = newton_nd_optimization_crude_step(formula, symbols, curr)\n",
    "        iterations += 1\n",
    "    x_list.append(curr)\n",
    "    return curr, iterations, x_list\n",
    "\n",
    "def sd_nd_optimization_crude(f_str, s_str, start, tolerance):\n",
    "    formula = sp.sympify(f_str)\n",
    "    symbols = sp.symbols(s_str)\n",
    "    curr = np.copy(start)\n",
    "    iterations = 0\n",
    "    gradient = np.inf\n",
    "    x_list = []\n",
    "    while (gradient > tolerance):\n",
    "        x_list.append(curr)\n",
    "        curr, gradient = sd_nd_optimization_crude_step(formula, symbols, curr)\n",
    "        iterations += 1\n",
    "    x_list.append(curr)\n",
    "    return curr, iterations, x_list\n",
    "\n",
    "\n",
    "r_newton, iteration_count_newton, newton_list = newton_nd_optimization_crude(f_str = '3 +((x**2)/8) + ((y**2)/8) - sin(x)*cos((2**-0.5)*y)', \n",
    "                                                                s_str = 'x y', \n",
    "                                                                start = r_init, \n",
    "                                                                tolerance = stop)\n",
    "\n",
    "r_sd, iteration_count_sd, sd_list = sd_nd_optimization_crude(f_str = '3 +((x**2)/8) + ((y**2)/8) - sin(x)*cos((2**-0.5)*y)', \n",
    "                                                    s_str = 'x y', \n",
    "                                                    start = r_init, \n",
    "                                                    tolerance = stop*10)\n",
    "\n",
    "\n",
    "newton_plot = [np.log(la.norm(r-r_newton,2)) for r in newton_list]\n",
    "sd_plot = [np.log(la.norm(r-r_sd,2)) for r in sd_list]\n",
    "\n",
    "plt.plot(sd_plot, label='Steepest descent line')\n",
    "plt.plot(newton_plot, label='newton')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('log(Error)')\n",
    "plt.legend()\n",
    "plt.title('Error vs. Iteration')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d30f2eae29d172888ee9c1ee28f70a4e8225fd9a8a138830c03a950ad069ae1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
